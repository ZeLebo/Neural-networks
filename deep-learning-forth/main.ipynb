{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gym[classic_control] in /Users/alexandersartakov/Library/Python/3.10/lib/python/site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/alexandersartakov/Library/Python/3.10/lib/python/site-packages (from gym[classic_control]) (1.24.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/alexandersartakov/Library/Python/3.10/lib/python/site-packages (from gym[classic_control]) (2.2.1)\n",
      "Requirement already satisfied: pyglet>=1.4.0 in /Users/alexandersartakov/Library/Python/3.10/lib/python/site-packages (from gym[classic_control]) (1.5.27)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: JSAnimation in /Users/alexandersartakov/Library/Python/3.10/lib/python/site-packages (0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install 'gym[classic_control]'\n",
    "!pip install JSAnimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from matplotlib import animation\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"Визуализируем гифочку из набора фрэймов\"\"\"\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    def display_animation(anim):\n",
    "        plt.close(anim._fig)\n",
    "        return HTML(anim.to_jshtml())\n",
    "\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    anim = animation.FuncAnimation(\n",
    "        plt.gcf(), animate, frames = len(frames), interval=50\n",
    "    )\n",
    "    \n",
    "    display(display_animation(anim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\" Простейший буффер реплеев \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim: int, size: int, batch_size: int = 32):\n",
    "        \"\"\"Initializate.\"\"\"\n",
    "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.acts_buf = np.zeros([size], dtype=np.float32)\n",
    "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
    "        self.done_buf = np.zeros([size], dtype=np.float32)\n",
    "        self.max_size, self.batch_size = size, batch_size\n",
    "        self.ptr, self.size, = 0, 0\n",
    "\n",
    "    def store(\n",
    "        self,\n",
    "        obs: np.ndarray,\n",
    "        act: np.ndarray, \n",
    "        rew: float, \n",
    "        next_obs: np.ndarray, \n",
    "        done: bool,\n",
    "    ):\n",
    "        \"\"\"Store the transition in buffer.\"\"\"\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.next_obs_buf[self.ptr] = next_obs\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
    "        return dict(obs=self.obs_buf[idxs],\n",
    "                    next_obs=self.next_obs_buf[idxs],\n",
    "                    acts=self.acts_buf[idxs],\n",
    "                    rews=self.rews_buf[idxs],\n",
    "                    done=self.done_buf[idxs])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\n",
    "\n",
    "    Класс для генерации данных с отклонениями для исследования пространства состояний\n",
    "    Выбранный шум хорошо подходит для физического управления с учетом инерции\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        size: int, \n",
    "        mu: float = 0.0, \n",
    "        theta: float = 0.15, \n",
    "        sigma: float = 0.2,\n",
    "    ):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.state = np.float64(0.0)\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self) -> np.ndarray:\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array(\n",
    "            [random.random() for _ in range(len(x))]\n",
    "        )\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_dim: int, \n",
    "        out_dim: int,\n",
    "        init_w: float = 3e-3,\n",
    "    ):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.hidden1 = nn.Linear(in_dim, 128)\n",
    "        self.hidden2 = nn.Linear(128, 128)\n",
    "        self.out = nn.Linear(128, out_dim)\n",
    "        \n",
    "        self.out.weight.data.uniform_(-init_w, init_w)\n",
    "        self.out.bias.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        x = F.relu(self.hidden1(state))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        action = self.out(x).tanh()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_dim: int, \n",
    "        init_w: float = 3e-3,\n",
    "    ):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.hidden1 = nn.Linear(in_dim, 128)\n",
    "        self.hidden2 = nn.Linear(128, 128)\n",
    "        self.out = nn.Linear(128, 1)\n",
    "        \n",
    "        self.out.weight.data.uniform_(-init_w, init_w)\n",
    "        self.out.bias.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(\n",
    "        self, state: torch.Tensor, action: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        x = torch.cat((state, action), dim=-1)\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        value = self.out(x)\n",
    "        \n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    \"\"\"DDPGAgent interacting with environment.\n",
    "    \n",
    "    Attribute:\n",
    "        env (gym.Env): openAI Gym environment\n",
    "        actor (nn.Module): target actor model to select actions\n",
    "        actor_target (nn.Module): actor model to predict next actions\n",
    "        actor_optimizer (Optimizer): optimizer for training actor\n",
    "        critic (nn.Module): critic model to predict state values\n",
    "        critic_target (nn.Module): target critic model to predict state values\n",
    "        critic_optimizer (Optimizer): optimizer for training critic\n",
    "        memory (ReplayBuffer): replay memory to store transitions\n",
    "        batch_size (int): batch size for sampling\n",
    "        gamma (float): discount factor\n",
    "        tau (float): parameter for soft target update\n",
    "        initial_random_steps (int): initial random action steps\n",
    "        noise (OUNoise): noise generator for exploration\n",
    "        device (torch.device): cpu / gpu\n",
    "        transition (list): temporory storage for the recent transition\n",
    "        total_step (int): total step numbers\n",
    "        is_test (bool): flag to show the current mode (train / test)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        memory_size: int,\n",
    "        batch_size: int,\n",
    "        ou_noise_theta: float,\n",
    "        ou_noise_sigma: float,\n",
    "        gamma: float = 0.99,\n",
    "        tau: float = 5e-3,\n",
    "        initial_random_steps: int = 1e4,\n",
    "    ):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        obs_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.shape[0]\n",
    "\n",
    "        self.env = env\n",
    "        self.memory = ReplayBuffer(obs_dim, memory_size, batch_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.initial_random_steps = initial_random_steps\n",
    "                \n",
    "        # noise\n",
    "        self.noise = OUNoise(\n",
    "            action_dim,\n",
    "            theta=ou_noise_theta,\n",
    "            sigma=ou_noise_sigma,\n",
    "        )\n",
    "\n",
    "        # device: cpu / gpu\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        print(self.device)\n",
    "\n",
    "        # networks\n",
    "        self.actor = Actor(obs_dim, action_dim).to(self.device)\n",
    "        self.actor_target = Actor(obs_dim, action_dim).to(self.device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        \n",
    "        self.critic = Critic(obs_dim + action_dim).to(self.device)\n",
    "        self.critic_target = Critic(obs_dim + action_dim).to(self.device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "        # optimizer\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "        \n",
    "        # transition to store in memory\n",
    "        self.transition = list()\n",
    "        \n",
    "        # total steps count\n",
    "        self.total_step = 0\n",
    "\n",
    "        # mode: train / test\n",
    "        self.is_test = False\n",
    "    \n",
    "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Select an action from the input state.\"\"\"\n",
    "        # if initial random action should be conducted\n",
    "        if self.total_step < self.initial_random_steps and not self.is_test:\n",
    "            selected_action = self.env.action_space.sample()\n",
    "        else:\n",
    "            selected_action = self.actor(\n",
    "                torch.FloatTensor(state).to(self.device)\n",
    "            ).detach().cpu().numpy()\n",
    "        \n",
    "        # add noise for exploration during training\n",
    "        if not self.is_test:\n",
    "            noise = self.noise.sample()\n",
    "            selected_action = np.clip(selected_action + noise, -1.0, 1.0)\n",
    "        \n",
    "        self.transition = [state, selected_action]\n",
    "        \n",
    "        return selected_action\n",
    "    \n",
    "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:\n",
    "        \"\"\"Take an action and return the response of the env.\"\"\"\n",
    "        next_state, reward, done, _ = self.env.step(action)\n",
    "        \n",
    "        if not self.is_test:\n",
    "            self.transition += [reward, next_state, done]\n",
    "            self.memory.store(*self.transition)\n",
    "    \n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def update_model(self) -> torch.Tensor:\n",
    "        \"\"\"Update the model by gradient descent.\"\"\"\n",
    "        device = self.device  # for shortening the following lines\n",
    "        \n",
    "        samples = self.memory.sample_batch()\n",
    "        state = torch.FloatTensor(samples[\"obs\"]).to(device)\n",
    "        next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n",
    "        action = torch.FloatTensor(samples[\"acts\"].reshape(-1, 1)).to(device)\n",
    "        reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n",
    "        done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n",
    "        \n",
    "        masks = 1 - done\n",
    "        next_action = self.actor_target(next_state)\n",
    "        next_value = self.critic_target(next_state, next_action)\n",
    "        curr_return = reward + self.gamma * next_value * masks\n",
    "        \n",
    "        # train critic\n",
    "        values = self.critic(state, action)\n",
    "        critic_loss = F.mse_loss(values, curr_return)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "                \n",
    "        # train actor\n",
    "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # target update\n",
    "        self._target_soft_update()\n",
    "        \n",
    "        return actor_loss.data, critic_loss.data\n",
    "    \n",
    "    def train(self, num_frames: int, plotting_interval: int = 200):\n",
    "        \"\"\"Train the agent.\"\"\"\n",
    "        self.is_test = False\n",
    "        \n",
    "        state = self.env.reset()\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        scores = []\n",
    "        score = 0\n",
    "        \n",
    "        for self.total_step in tqdm(range(1, num_frames + 1)):\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, done = self.step(action)\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "            # if episode ends\n",
    "            if done:         \n",
    "                state = env.reset()\n",
    "                scores.append(score)\n",
    "                score = 0\n",
    "\n",
    "            # if training is ready\n",
    "            if (\n",
    "                len(self.memory) >= self.batch_size \n",
    "                and self.total_step > self.initial_random_steps\n",
    "            ):\n",
    "                actor_loss, critic_loss = self.update_model()\n",
    "                actor_losses.append(actor_loss.cpu())\n",
    "                critic_losses.append(critic_loss.cpu())\n",
    "            \n",
    "            # plotting\n",
    "            if self.total_step % plotting_interval == 0:\n",
    "                self._plot(\n",
    "                    self.total_step, \n",
    "                    scores, \n",
    "                    actor_losses, \n",
    "                    critic_losses,\n",
    "                )\n",
    "                \n",
    "        self.env.close()\n",
    "        \n",
    "    def test(self):\n",
    "        \"\"\"Test the agent.\"\"\"\n",
    "        self.is_test = True\n",
    "        \n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        \n",
    "        frames = []\n",
    "        while not done:\n",
    "            frames.append(self.env.render(mode=\"rgb_array\"))\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, done = self.step(action)\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "        \n",
    "        print(\"score: \", score)\n",
    "        self.env.close()\n",
    "        \n",
    "        return frames\n",
    "    \n",
    "    def _target_soft_update(self):\n",
    "        \"\"\"Soft-update: target = tau*local + (1-tau)*target.\"\"\"\n",
    "        tau = self.tau\n",
    "        \n",
    "        for t_param, l_param in zip(\n",
    "            self.actor_target.parameters(), self.actor.parameters()\n",
    "        ):\n",
    "            t_param.data.copy_(tau * l_param.data + (1.0 - tau) * t_param.data)\n",
    "            \n",
    "        for t_param, l_param in zip(\n",
    "            self.critic_target.parameters(), self.critic.parameters()\n",
    "        ):\n",
    "            t_param.data.copy_(tau * l_param.data + (1.0 - tau) * t_param.data)\n",
    "    \n",
    "    def _plot(\n",
    "        self, \n",
    "        frame_idx: int, \n",
    "        scores: List[float], \n",
    "        actor_losses: List[float], \n",
    "        critic_losses: List[float], \n",
    "    ):\n",
    "        \"\"\"Plot the training progresses.\"\"\"\n",
    "        def subplot(loc: int, title: str, values: List[float]):\n",
    "            plt.subplot(loc)\n",
    "            plt.title(title)\n",
    "            plt.plot(values)\n",
    "\n",
    "        subplot_params = [\n",
    "            (131, f\"frame {frame_idx}. score: {np.mean(scores[-10:])}\", scores),\n",
    "            (132, \"actor_loss\", actor_losses),\n",
    "            (133, \"critic_loss\", critic_losses),\n",
    "        ]\n",
    "        \n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(30, 5))\n",
    "        for loc, title, values in subplot_params:\n",
    "            subplot(loc, title, values)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionNormalizer(gym.ActionWrapper):\n",
    "    \"\"\"Rescale and relocate the actions.\"\"\"\n",
    "\n",
    "    def action(self, action: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Change the range (-1, 1) to (low, high).\"\"\"\n",
    "        low = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "\n",
    "        scale_factor = (high - low) / 2\n",
    "        reloc_factor = high - scale_factor\n",
    "\n",
    "        action = action * scale_factor + reloc_factor\n",
    "        action = np.clip(action, low, high)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def reverse_action(self, action: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Change the range (low, high) to (-1, 1).\"\"\"\n",
    "        low = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "\n",
    "        scale_factor = (high - low) / 2\n",
    "        reloc_factor = high - scale_factor\n",
    "\n",
    "        action = (action - reloc_factor) / scale_factor\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "# env_id = \"Pendulum-v1\"\n",
    "# env = gym.make(env_id)\n",
    "# env = ActionNormalizer(env)\n",
    "\n",
    "# print(f\"Observation space: {env.observation_space.shape[0]}\")\n",
    "# print(f\"Action dim: {env.action_space.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 42\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# seed_torch(seed)\n",
    "# env.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "# memory_size = 100000\n",
    "# batch_size = 128\n",
    "# ou_noise_theta = 1.0\n",
    "# ou_noise_sigma = 0.1\n",
    "# initial_random_steps = 10000\n",
    "\n",
    "# agent = DDPGAgent(\n",
    "#     env, \n",
    "#     memory_size, \n",
    "#     batch_size,\n",
    "#     ou_noise_theta,\n",
    "#     ou_noise_sigma,\n",
    "#     initial_random_steps=initial_random_steps\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test without training\n",
    "# frames = agent.test()\n",
    "# display\n",
    "# display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test after 1000\n",
    "\n",
    "# num_frames = initial_random_steps + 1000\n",
    "\n",
    "# agent = DDPGAgent(\n",
    "#     env, \n",
    "#     memory_size, \n",
    "#     batch_size,\n",
    "#     ou_noise_theta,\n",
    "#     ou_noise_sigma,\n",
    "#     initial_random_steps=initial_random_steps\n",
    "# )\n",
    "# agent.train(num_frames)\n",
    "\n",
    "# frames = agent.test()\n",
    "# display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 20\u001b[0m\n\u001b[1;32m      8\u001b[0m     agent \u001b[39m=\u001b[39m DQNAgent(\n\u001b[1;32m      9\u001b[0m         env,\n\u001b[1;32m     10\u001b[0m         memory_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m         initial_random_steps,\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     19\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m     agent \u001b[39m=\u001b[39m DDPGAgent(\n\u001b[1;32m     21\u001b[0m         env,\n\u001b[1;32m     22\u001b[0m         memory_size,\n\u001b[1;32m     23\u001b[0m         batch_size,\n\u001b[1;32m     24\u001b[0m         ou_noise_theta,\n\u001b[1;32m     25\u001b[0m         ou_noise_sigma,\n\u001b[1;32m     26\u001b[0m         initial_random_steps\u001b[39m=\u001b[39;49minitial_random_steps\n\u001b[1;32m     27\u001b[0m     )\n\u001b[1;32m     29\u001b[0m \u001b[39m# test without training\u001b[39;00m\n\u001b[1;32m     30\u001b[0m frames \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mtest()\n",
      "Cell \u001b[0;32mIn[7], line 36\u001b[0m, in \u001b[0;36mDDPGAgent.__init__\u001b[0;34m(self, env, memory_size, batch_size, ou_noise_theta, ou_noise_sigma, gamma, tau, initial_random_steps)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initialize.\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m obs_dim \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 36\u001b[0m action_dim \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49maction_space\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]\n\u001b[1;32m     38\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39m=\u001b[39m env\n\u001b[1;32m     39\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory \u001b[39m=\u001b[39m ReplayBuffer(obs_dim, memory_size, batch_size)\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"ALE/Breakout-v5\")\n",
    "env = ActionNormalizer(env)\n",
    "\n",
    "# if env.activation_space is none then it is discrete action space so we can use DQN\n",
    "# if env.activation_space is not none then it is continuous action space so we can use DDPG\n",
    "\n",
    "if env.action_space is None:\n",
    "    agent = DQNAgent(\n",
    "        env,\n",
    "        memory_size,\n",
    "        batch_size,\n",
    "        epsilon_decay_steps,\n",
    "        epsilon_start,\n",
    "        epsilon_final,\n",
    "        gamma,\n",
    "        target_update_steps,\n",
    "        initial_random_steps,\n",
    "    )\n",
    "else:\n",
    "    agent = DDPGAgent(\n",
    "        env,\n",
    "        memory_size,\n",
    "        batch_size,\n",
    "        ou_noise_theta,\n",
    "        ou_noise_sigma,\n",
    "        initial_random_steps=initial_random_steps\n",
    "    )\n",
    "\n",
    "# test without training\n",
    "frames = agent.test()\n",
    "display_frames_as_gif(frames)\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"Observation space: {env.observation_space.shape[0]}\")\n",
    "# print(f\"Action dim: {env.action_space.shape[0]}\")\n",
    "\n",
    "# seed = 42\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# seed_torch(seed)\n",
    "# env.seed(seed)\n",
    "\n",
    "# memory_size = 100000\n",
    "# batch_size = 128\n",
    "# ou_noise_theta = 1.0\n",
    "# ou_noise_sigma = 0.1\n",
    "# initial_random_steps = 10000\n",
    "\n",
    "# agent = DDPGAgent(\n",
    "#     env,\n",
    "#     memory_size,\n",
    "#     batch_size,\n",
    "#     ou_noise_theta,\n",
    "#     ou_noise_sigma,\n",
    "#     initial_random_steps=initial_random_steps\n",
    "# )\n",
    "\n",
    "# # test without training\n",
    "# frames = agent.test()\n",
    "# display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# train for 1000\u001b[39;00m\n\u001b[1;32m      2\u001b[0m num_frames \u001b[39m=\u001b[39m initial_random_steps \u001b[39m+\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[0;32m----> 4\u001b[0m agent \u001b[39m=\u001b[39m DDPGAgent(\n\u001b[1;32m      5\u001b[0m     env,\n\u001b[1;32m      6\u001b[0m     memory_size,\n\u001b[1;32m      7\u001b[0m     batch_size,\n\u001b[1;32m      8\u001b[0m     ou_noise_theta,\n\u001b[1;32m      9\u001b[0m     ou_noise_sigma,\n\u001b[1;32m     10\u001b[0m     initial_random_steps\u001b[39m=\u001b[39;49minitial_random_steps\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m agent\u001b[39m.\u001b[39mtrain(num_frames)\n\u001b[1;32m     14\u001b[0m frames \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mtest()\n",
      "Cell \u001b[0;32mIn[7], line 36\u001b[0m, in \u001b[0;36mDDPGAgent.__init__\u001b[0;34m(self, env, memory_size, batch_size, ou_noise_theta, ou_noise_sigma, gamma, tau, initial_random_steps)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initialize.\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m obs_dim \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 36\u001b[0m action_dim \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49maction_space\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]\n\u001b[1;32m     38\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39m=\u001b[39m env\n\u001b[1;32m     39\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory \u001b[39m=\u001b[39m ReplayBuffer(obs_dim, memory_size, batch_size)\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# train for 1000\n",
    "num_frames = initial_random_steps + 1000\n",
    "\n",
    "agent = DDPGAgent(\n",
    "    env,\n",
    "    memory_size,\n",
    "    batch_size,\n",
    "    ou_noise_theta,\n",
    "    ou_noise_sigma,\n",
    "    initial_random_steps=initial_random_steps\n",
    ")\n",
    "agent.train(num_frames)\n",
    "\n",
    "frames = agent.test()\n",
    "display_frames_as_gif(frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
