{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "import os\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "log_path = \"Logs\"\n",
    "model_path = \"Saved Models\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "env = make_atari_env(\"ALE/MsPacman-v5\", n_envs=4, seed=SEED)\n",
    "env = VecFrameStack(env, n_stack=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False  True]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False  True False False]\n",
      "[False False False False]\n",
      "[ True False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False  True False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False  True]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False  True]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False  True False]\n",
      "[False False False False]\n",
      "[False False False  True]\n",
      "[False False False False]\n",
      "[False  True False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False  True False]\n",
      "[ True False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False  True]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False  True False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False  True False]\n",
      "[ True False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False  True]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[ True  True  True False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False  True]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False  True False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[ True False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False  True False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False  True]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False  True False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[ True False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False  True False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False  True]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[ True False False False]\n",
      "[False False  True False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False  True False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False  True False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[ True False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False  True]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False  True False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False  True False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[ True False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False  True]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False  True False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[ True False False False]\n",
      "[False False False False]\n",
      "[False False  True False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False  True False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False  True]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False  True False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False  True False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[ True False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False  True]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False  True False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[ True False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False  True False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False  True]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[ True False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False  True False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False  True False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[ True False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False False False False]\n",
      "[False  True False False]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [5], line 9\u001B[0m\n\u001B[0;32m      7\u001B[0m env\u001B[38;5;241m.\u001B[39mrender()\n\u001B[0;32m      8\u001B[0m action \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39maction_space\u001B[38;5;241m.\u001B[39msample()\n\u001B[1;32m----> 9\u001B[0m n_state, reward, done, info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep([action]\u001B[38;5;241m*\u001B[39mos\u001B[38;5;241m.\u001B[39mcpu_count())\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(done)\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:163\u001B[0m, in \u001B[0;36mVecEnv.step\u001B[1;34m(self, actions)\u001B[0m\n\u001B[0;32m    156\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;124;03mStep the environments with the given action\u001B[39;00m\n\u001B[0;32m    158\u001B[0m \n\u001B[0;32m    159\u001B[0m \u001B[38;5;124;03m:param actions: the action\u001B[39;00m\n\u001B[0;32m    160\u001B[0m \u001B[38;5;124;03m:return: observation, reward, done, information\u001B[39;00m\n\u001B[0;32m    161\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    162\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep_async(actions)\n\u001B[1;32m--> 163\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep_wait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_frame_stack.py:33\u001B[0m, in \u001B[0;36mVecFrameStack.step_wait\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep_wait\u001B[39m(\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m     32\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Union[np\u001B[38;5;241m.\u001B[39mndarray, Dict[\u001B[38;5;28mstr\u001B[39m, np\u001B[38;5;241m.\u001B[39mndarray]], np\u001B[38;5;241m.\u001B[39mndarray, np\u001B[38;5;241m.\u001B[39mndarray, List[Dict[\u001B[38;5;28mstr\u001B[39m, Any]],]:\n\u001B[1;32m---> 33\u001B[0m     observations, rewards, dones, infos \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvenv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep_wait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     34\u001B[0m     observations, infos \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstacked_obs\u001B[38;5;241m.\u001B[39mupdate(observations, dones, infos)\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m observations, rewards, dones, infos\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:54\u001B[0m, in \u001B[0;36mDummyVecEnv.step_wait\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep_wait\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m VecEnvStepReturn:\n\u001B[0;32m     53\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m env_idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_envs):\n\u001B[1;32m---> 54\u001B[0m         obs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_rews[env_idx], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_dones[env_idx], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_infos[env_idx] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menvs\u001B[49m\u001B[43m[\u001B[49m\u001B[43menv_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mactions\u001B[49m\u001B[43m[\u001B[49m\u001B[43menv_idx\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     57\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_dones[env_idx]:\n\u001B[0;32m     58\u001B[0m             \u001B[38;5;66;03m# save final observation where user can get it, then reset\u001B[39;00m\n\u001B[0;32m     59\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_infos[env_idx][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mterminal_observation\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m obs\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\gym\\core.py:300\u001B[0m, in \u001B[0;36mWrapper.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    299\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[1;32m--> 300\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\gym\\core.py:347\u001B[0m, in \u001B[0;36mRewardWrapper.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[1;32m--> 347\u001B[0m     observation, reward, done, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    348\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m observation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreward(reward), done, info\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\gym\\core.py:334\u001B[0m, in \u001B[0;36mObservationWrapper.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    333\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[1;32m--> 334\u001B[0m     observation, reward, done, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    335\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation(observation), reward, done, info\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\stable_baselines3\\common\\atari_wrappers.py:109\u001B[0m, in \u001B[0;36mEpisodicLifeEnv.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    108\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m GymStepReturn:\n\u001B[1;32m--> 109\u001B[0m     obs, reward, done, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    110\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwas_real_done \u001B[38;5;241m=\u001B[39m done\n\u001B[0;32m    111\u001B[0m     \u001B[38;5;66;03m# check current lives, make loss of life terminal,\u001B[39;00m\n\u001B[0;32m    112\u001B[0m     \u001B[38;5;66;03m# then update lives to handle bonus lives\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\stable_baselines3\\common\\atari_wrappers.py:176\u001B[0m, in \u001B[0;36mMaxAndSkipEnv.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_skip \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[0;32m    175\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_obs_buffer[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m obs\n\u001B[1;32m--> 176\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_skip\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m:\n\u001B[0;32m    177\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_obs_buffer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m obs\n\u001B[0;32m    178\u001B[0m total_reward \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m reward\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = np.array([False])\n",
    "\n",
    "    while not done.all():\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step([action]*os.cpu_count())\n",
    "        print(done)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"CnnPolicy\", env, verbose=1, tensorboard_log=log_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training2\\Logs\\PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 559      |\n",
      "|    ep_rew_mean     | 487      |\n",
      "| time/              |          |\n",
      "|    fps             | 173      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 47       |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 550        |\n",
      "|    ep_rew_mean          | 469        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 96         |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00800604 |\n",
      "|    clip_fraction        | 0.0799     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.19      |\n",
      "|    explained_variance   | -0.00486   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.68       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.00163   |\n",
      "|    value_loss           | 4.98       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 538         |\n",
      "|    ep_rew_mean          | 462         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 143         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009043973 |\n",
      "|    clip_fraction        | 0.0947      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.19       |\n",
      "|    explained_variance   | 0.415       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.17        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00478    |\n",
      "|    value_loss           | 5.51        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 566         |\n",
      "|    ep_rew_mean          | 548         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 172         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 190         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011091139 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.18       |\n",
      "|    explained_variance   | 0.507       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.65        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00617    |\n",
      "|    value_loss           | 5.68        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 547         |\n",
      "|    ep_rew_mean          | 533         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 172         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 237         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012631616 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.16       |\n",
      "|    explained_variance   | 0.522       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.96        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00838    |\n",
      "|    value_loss           | 4.66        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 556         |\n",
      "|    ep_rew_mean          | 532         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 173         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 283         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012678476 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.13       |\n",
      "|    explained_variance   | 0.562       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.06        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00915    |\n",
      "|    value_loss           | 4.49        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 556         |\n",
      "|    ep_rew_mean          | 532         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 173         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 330         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015586703 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.11       |\n",
      "|    explained_variance   | 0.586       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.09        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    value_loss           | 3.09        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 569         |\n",
      "|    ep_rew_mean          | 595         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 173         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 377         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016251937 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.09       |\n",
      "|    explained_variance   | 0.546       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.14        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    value_loss           | 2.95        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 590         |\n",
      "|    ep_rew_mean          | 640         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 173         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 424         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017711103 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.09       |\n",
      "|    explained_variance   | 0.579       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.811       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    value_loss           | 2.78        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 628         |\n",
      "|    ep_rew_mean          | 692         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 173         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 472         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018835943 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.05       |\n",
      "|    explained_variance   | 0.636       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.07        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    value_loss           | 2.47        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 594         |\n",
      "|    ep_rew_mean          | 642         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 173         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 519         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020124145 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.05       |\n",
      "|    explained_variance   | 0.692       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.397       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 2.01        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 597         |\n",
      "|    ep_rew_mean          | 637         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 173         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 565         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021942493 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.04       |\n",
      "|    explained_variance   | 0.655       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.516       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0214     |\n",
      "|    value_loss           | 2.62        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 619         |\n",
      "|    ep_rew_mean          | 670         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 174         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 611         |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023167202 |\n",
      "|    clip_fraction        | 0.296       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.02       |\n",
      "|    explained_variance   | 0.643       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.458       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    value_loss           | 2.23        |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(os.path.join(model_path, 'PPO_100k.zip')):\n",
    "    model = PPO.load(os.path.join(model_path, 'PPO_100k.zip'), env=env)\n",
    "else:\n",
    "    model.learn(100000)\n",
    "    model.save(os.path.join(model_path, 'PPO_100k'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "(333.0, 194.21894861212692)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model=model, env=env, render=True, n_eval_episodes=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "env.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training2\\Logs\\PPO_2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [13], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m     model \u001B[38;5;241m=\u001B[39m PPO\u001B[38;5;241m.\u001B[39mload(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(model_path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPPO_500k.zip\u001B[39m\u001B[38;5;124m'\u001B[39m), env\u001B[38;5;241m=\u001B[39menv)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m----> 4\u001B[0m     model\u001B[38;5;241m.\u001B[39mlearn(\u001B[38;5;241m500000\u001B[39m)\n\u001B[0;32m      5\u001B[0m     model\u001B[38;5;241m.\u001B[39msave(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(model_path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPPO_500k\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:308\u001B[0m, in \u001B[0;36mPPO.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[0;32m    299\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlearn\u001B[39m(\n\u001B[0;32m    300\u001B[0m     \u001B[38;5;28mself\u001B[39m: SelfPPO,\n\u001B[0;32m    301\u001B[0m     total_timesteps: \u001B[38;5;28mint\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    306\u001B[0m     progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    307\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m SelfPPO:\n\u001B[1;32m--> 308\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    309\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    310\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    311\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlog_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_interval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    312\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_log_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    313\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    314\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    315\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:250\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[0;32m    247\u001B[0m callback\u001B[38;5;241m.\u001B[39mon_training_start(\u001B[38;5;28mlocals\u001B[39m(), \u001B[38;5;28mglobals\u001B[39m())\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps \u001B[38;5;241m<\u001B[39m total_timesteps:\n\u001B[1;32m--> 250\u001B[0m     continue_training \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect_rollouts\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrollout_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_rollout_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_steps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    252\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m continue_training \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[0;32m    253\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:169\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.collect_rollouts\u001B[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001B[0m\n\u001B[0;32m    166\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m th\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m    167\u001B[0m     \u001B[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001B[39;00m\n\u001B[0;32m    168\u001B[0m     obs_tensor \u001B[38;5;241m=\u001B[39m obs_as_tensor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_last_obs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m--> 169\u001B[0m     actions, values, log_probs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs_tensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    170\u001B[0m actions \u001B[38;5;241m=\u001B[39m actions\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m    172\u001B[0m \u001B[38;5;66;03m# Rescale and perform action\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\stable_baselines3\\common\\policies.py:623\u001B[0m, in \u001B[0;36mActorCriticPolicy.forward\u001B[1;34m(self, obs, deterministic)\u001B[0m\n\u001B[0;32m    621\u001B[0m \u001B[38;5;66;03m# Evaluate the values for the given observations\u001B[39;00m\n\u001B[0;32m    622\u001B[0m values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalue_net(latent_vf)\n\u001B[1;32m--> 623\u001B[0m distribution \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_action_dist_from_latent\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlatent_pi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    624\u001B[0m actions \u001B[38;5;241m=\u001B[39m distribution\u001B[38;5;241m.\u001B[39mget_actions(deterministic\u001B[38;5;241m=\u001B[39mdeterministic)\n\u001B[0;32m    625\u001B[0m log_prob \u001B[38;5;241m=\u001B[39m distribution\u001B[38;5;241m.\u001B[39mlog_prob(actions)\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\stable_baselines3\\common\\policies.py:656\u001B[0m, in \u001B[0;36mActorCriticPolicy._get_action_dist_from_latent\u001B[1;34m(self, latent_pi)\u001B[0m\n\u001B[0;32m    653\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_dist\u001B[38;5;241m.\u001B[39mproba_distribution(mean_actions, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog_std)\n\u001B[0;32m    654\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_dist, CategoricalDistribution):\n\u001B[0;32m    655\u001B[0m     \u001B[38;5;66;03m# Here mean_actions are the logits before the softmax\u001B[39;00m\n\u001B[1;32m--> 656\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maction_dist\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproba_distribution\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction_logits\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmean_actions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    657\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_dist, MultiCategoricalDistribution):\n\u001B[0;32m    658\u001B[0m     \u001B[38;5;66;03m# Here mean_actions are the flattened logits\u001B[39;00m\n\u001B[0;32m    659\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_dist\u001B[38;5;241m.\u001B[39mproba_distribution(action_logits\u001B[38;5;241m=\u001B[39mmean_actions)\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\stable_baselines3\\common\\distributions.py:288\u001B[0m, in \u001B[0;36mCategoricalDistribution.proba_distribution\u001B[1;34m(self, action_logits)\u001B[0m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mproba_distribution\u001B[39m(\u001B[38;5;28mself\u001B[39m: SelfCategoricalDistribution, action_logits: th\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m SelfCategoricalDistribution:\n\u001B[1;32m--> 288\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistribution \u001B[38;5;241m=\u001B[39m \u001B[43mCategorical\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogits\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction_logits\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    289\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\torch\\distributions\\categorical.py:66\u001B[0m, in \u001B[0;36mCategorical.__init__\u001B[1;34m(self, probs, logits, validate_args)\u001B[0m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_events \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_param\u001B[38;5;241m.\u001B[39msize()[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m     65\u001B[0m batch_shape \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_param\u001B[38;5;241m.\u001B[39msize()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_param\u001B[38;5;241m.\u001B[39mndimension() \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mSize()\n\u001B[1;32m---> 66\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mbatch_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidate_args\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidate_args\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\torch\\distributions\\distribution.py:60\u001B[0m, in \u001B[0;36mDistribution.__init__\u001B[1;34m(self, batch_shape, event_shape, validate_args)\u001B[0m\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m  \u001B[38;5;66;03m# skip checking lazily-constructed args\u001B[39;00m\n\u001B[0;32m     59\u001B[0m value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, param)\n\u001B[1;32m---> 60\u001B[0m valid \u001B[38;5;241m=\u001B[39m \u001B[43mconstraint\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m valid\u001B[38;5;241m.\u001B[39mall():\n\u001B[0;32m     62\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m     63\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected parameter \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparam\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     64\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(value)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m of shape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtuple\u001B[39m(value\u001B[38;5;241m.\u001B[39mshape)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     67\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut found invalid values:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mvalue\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     68\u001B[0m     )\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\torch\\distributions\\constraints.py:209\u001B[0m, in \u001B[0;36m_IndependentConstraint.check\u001B[1;34m(self, value)\u001B[0m\n\u001B[0;32m    208\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcheck\u001B[39m(\u001B[38;5;28mself\u001B[39m, value):\n\u001B[1;32m--> 209\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbase_constraint\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    210\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m result\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreinterpreted_batch_ndims:\n\u001B[0;32m    211\u001B[0m         expected \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_constraint\u001B[38;5;241m.\u001B[39mevent_dim \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreinterpreted_batch_ndims\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=2000, verbose=1)\n",
    "eval_callback = EvalCallback(\n",
    "    env,\n",
    "    callback_on_new_best=stop_callback,\n",
    "    eval_freq=10000,\n",
    "    best_model_save_path=model_path,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "if os.path.exists(os.path.join(model_path, 'PPO_500k.zip')):\n",
    "    model = PPO.load(os.path.join(model_path, 'PPO_500k.zip'), env=env, )\n",
    "else:\n",
    "    model.learn(500000, callback=eval_callback)\n",
    "    model.save(os.path.join(model_path, 'PPO_500k'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "(14.5, 6.606814663663572)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model=model, env=env, render=True, n_eval_episodes=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "стоило сразу коллбэк повесить, а то произошли сильные просадки про скору и надо вытащить из этой ямы"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "env.reset()\n",
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=20, verbose=1)\n",
    "eval_callback = EvalCallback(\n",
    "    env,\n",
    "    callback_on_new_best=stop_callback,\n",
    "    eval_freq=10000,\n",
    "    best_model_save_path=model_path,\n",
    "    verbose=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\stable_baselines3\\common\\callbacks.py:403: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x000001A759F36A10> != <stable_baselines3.common.vec_env.vec_frame_stack.VecFrameStack object at 0x000001A5B2F1AFB0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 700      |\n",
      "|    ep_rew_mean     | 17       |\n",
      "| time/              |          |\n",
      "|    fps             | 219      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 37       |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 665       |\n",
      "|    ep_rew_mean          | 15.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 181       |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 90        |\n",
      "|    total_timesteps      | 16384     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4739682 |\n",
      "|    clip_fraction        | 0.439     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.457    |\n",
      "|    explained_variance   | 0.579     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0371   |\n",
      "|    n_updates            | 1370      |\n",
      "|    policy_gradient_loss | -0.0674   |\n",
      "|    value_loss           | 0.215     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 661       |\n",
      "|    ep_rew_mean          | 15.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 173       |\n",
      "|    iterations           | 3         |\n",
      "|    time_elapsed         | 141       |\n",
      "|    total_timesteps      | 24576     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4109888 |\n",
      "|    clip_fraction        | 0.439     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.501    |\n",
      "|    explained_variance   | 0.659     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.000885 |\n",
      "|    n_updates            | 1380      |\n",
      "|    policy_gradient_loss | -0.0657   |\n",
      "|    value_loss           | 0.192     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 669        |\n",
      "|    ep_rew_mean          | 16.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 193        |\n",
      "|    total_timesteps      | 32768      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.45296538 |\n",
      "|    clip_fraction        | 0.443      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.487     |\n",
      "|    explained_variance   | 0.685      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.064     |\n",
      "|    n_updates            | 1390       |\n",
      "|    policy_gradient_loss | -0.0656    |\n",
      "|    value_loss           | 0.155      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=15.40 +/- 4.59\n",
      "Episode length: 681.60 +/- 128.11\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 682        |\n",
      "|    mean_reward          | 15.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 40000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.43245804 |\n",
      "|    clip_fraction        | 0.444      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.497     |\n",
      "|    explained_variance   | 0.706      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0936    |\n",
      "|    n_updates            | 1400       |\n",
      "|    policy_gradient_loss | -0.0709    |\n",
      "|    value_loss           | 0.154      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 675      |\n",
      "|    ep_rew_mean     | 16.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 164      |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 249      |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 653        |\n",
      "|    ep_rew_mean          | 15.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 162        |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 301        |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.42279252 |\n",
      "|    clip_fraction        | 0.448      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.5       |\n",
      "|    explained_variance   | 0.738      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0594    |\n",
      "|    n_updates            | 1410       |\n",
      "|    policy_gradient_loss | -0.0694    |\n",
      "|    value_loss           | 0.157      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 654       |\n",
      "|    ep_rew_mean          | 15.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 162       |\n",
      "|    iterations           | 7         |\n",
      "|    time_elapsed         | 353       |\n",
      "|    total_timesteps      | 57344     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4689479 |\n",
      "|    clip_fraction        | 0.444     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.477    |\n",
      "|    explained_variance   | 0.72      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0168   |\n",
      "|    n_updates            | 1420      |\n",
      "|    policy_gradient_loss | -0.069    |\n",
      "|    value_loss           | 0.133     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 674       |\n",
      "|    ep_rew_mean          | 15.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 161       |\n",
      "|    iterations           | 8         |\n",
      "|    time_elapsed         | 406       |\n",
      "|    total_timesteps      | 65536     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5126863 |\n",
      "|    clip_fraction        | 0.437     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.462    |\n",
      "|    explained_variance   | 0.705     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0116   |\n",
      "|    n_updates            | 1430      |\n",
      "|    policy_gradient_loss | -0.0687   |\n",
      "|    value_loss           | 0.153     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 680       |\n",
      "|    ep_rew_mean          | 16.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 161       |\n",
      "|    iterations           | 9         |\n",
      "|    time_elapsed         | 457       |\n",
      "|    total_timesteps      | 73728     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5355576 |\n",
      "|    clip_fraction        | 0.451     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.472    |\n",
      "|    explained_variance   | 0.717     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0519   |\n",
      "|    n_updates            | 1440      |\n",
      "|    policy_gradient_loss | -0.0669   |\n",
      "|    value_loss           | 0.177     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=13.20 +/- 4.31\n",
      "Episode length: 613.20 +/- 141.19\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 613        |\n",
      "|    mean_reward          | 13.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 80000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.46834362 |\n",
      "|    clip_fraction        | 0.43       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.46      |\n",
      "|    explained_variance   | 0.713      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0676    |\n",
      "|    n_updates            | 1450       |\n",
      "|    policy_gradient_loss | -0.0652    |\n",
      "|    value_loss           | 0.15       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 666      |\n",
      "|    ep_rew_mean     | 16.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 159      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 512      |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 647        |\n",
      "|    ep_rew_mean          | 15.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 159        |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 565        |\n",
      "|    total_timesteps      | 90112      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.42983985 |\n",
      "|    clip_fraction        | 0.42       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.479     |\n",
      "|    explained_variance   | 0.743      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0373    |\n",
      "|    n_updates            | 1460       |\n",
      "|    policy_gradient_loss | -0.0612    |\n",
      "|    value_loss           | 0.199      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 657        |\n",
      "|    ep_rew_mean          | 15.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 159        |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 618        |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.44603828 |\n",
      "|    clip_fraction        | 0.449      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.481     |\n",
      "|    explained_variance   | 0.715      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0941    |\n",
      "|    n_updates            | 1470       |\n",
      "|    policy_gradient_loss | -0.0667    |\n",
      "|    value_loss           | 0.155      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 700        |\n",
      "|    ep_rew_mean          | 17.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 159        |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 669        |\n",
      "|    total_timesteps      | 106496     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.49917412 |\n",
      "|    clip_fraction        | 0.439      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.462     |\n",
      "|    explained_variance   | 0.677      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0858    |\n",
      "|    n_updates            | 1480       |\n",
      "|    policy_gradient_loss | -0.0644    |\n",
      "|    value_loss           | 0.178      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "<stable_baselines3.ppo.ppo.PPO at 0x1a588056ad0>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(100000, callback=eval_callback)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "data": {
      "text/plain": "(17.92, 6.045957327007858)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "best = PPO.load(os.path.join(model_path, 'best_model.zip'), env=env)\n",
    "evaluate_policy(model=best, env=env, render=True, n_eval_episodes=50)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
